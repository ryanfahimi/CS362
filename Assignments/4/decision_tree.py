import pandas as pd
import math
from collections import Counter


def zero_r(data):
    c = Counter(data)
    return c.most_common(1)[0][0]


class Node:
    def __init__(self, classification=None, attribute=None):
        self.classification = classification
        self.attribute = attribute
        self.children = {}

    def is_leaf(self):
        return len(self.children) == 0


# assume that s is a pandas series.
# return the entropy of the series
def entropy(s):
    counts = s.value_counts()
    e = sum([item / sum(counts) * math.log(item / sum(counts), 2) * -1 for item in counts])
    return e


# gain takes two inputs: a Series containing
# the feature (such as outlook) and a Series
# containing the classification such as 'play'.
# gain should compute the entropy for each feature value and then return
# their weighted average.
def gain(features, classifications):
    unique_features = set(features)
    initial_entropy = entropy(classifications)
    remainder = 0
    vars = list(zip(list(features), list(classifications)))
    for unique_feature in unique_features:
        feature_classifications = [classification for feature, classification in vars if feature == unique_feature]
        var_entropy = entropy(pd.Series(feature_classifications))
        remainder += var_entropy * (len(feature_classifications) / len(classifications))
    return initial_entropy - remainder


# select_attribute takes two inputs:
# 1. A dataframe consisting of the features to be considered
# 2. A Series containing the corresponding classifications.
# select_attribute computes the gain for each feature in the dataframe
# and returns the name of the column with the greatest gain.
def select_attribute(features, classifications):
    gains = {col: gain(features[col], classifications) for col in features.columns}
    return max(gains, key=gains.get)


# make_tree takes two required inputs
# 1. A dataframe consisting of the features to be considered
# 2. A Series containing the corresponding classifications.
# there are also two optional arguments:
# 1. zero_r_val = the value generated by ZeroR
# 2 attr_dict  = a dictionary mapping variable names (such as outlook)
#   to a list of values (such as ['sunny','overcast','rainy']
# It's a recursive method.
# There are three base cases:
# 1. All the elements of classifications are the same.
#    We are at a leaf node. Create the node with this value as the classification
#    and return it.
# 2. There are no rows in the dataframe. In this case, we have no observations
#    for this set of feature values. So we are at a leaf. Create and return
#    a leaf node using ZeroR to set the value.
# 3. There are no columns in the dataframe. In this case, we had either
#    noise or missing observations preventing us from completely separating
#    the data. So we are at a leaf. Create and return
#    a leaf node using ZeroR to set the value.
# Then there is a recursive step:
#     use select_attribute to find the best attribute to split on.
#     construct a non-leaf node
#     For each value of that attribute, extract the rows that have that classification,
#     along with the corresponding classifications into a new dataframe and series.
#     Drop the column for this feature from these new Dataframes so we don't retest it.
#     Call make_tree with this new Dataframe and series and add the node that's
#     returned to the children, with key = feature name and value = Node.
def make_tree(features, classifications, zero_r_val='play', attr_dict=None):
    if len(set(classifications)) == 1:
        return Node(classifications.iloc[0])
    if len(features) == 0 or len(features.columns) == 0:
        return Node(zero_r_val)
    best_feature = select_attribute(features, classifications)
    node = Node(attribute=best_feature)
    for val in attr_dict[best_feature]:
        new_features = features[features[best_feature] == val].drop(best_feature, axis=1)
        new_classifications = classifications[features[best_feature] == val]
        node.children[val] = make_tree(new_features, new_classifications, zero_r_val, attr_dict)
    return node


# classify takes two inputs:
# A node and a Series representing the data to classify.
# This is also a recursive function.
# Base case:
#   We are at a leaf. Return the value stored in classification.
# Recursive step:
#   Check what attribute node tests.
#   Get the value of data_to_classify for that feature.
#   Call classify on the child node corresponding to that value.
def classify(tree, data_to_classify):
    if tree.is_leaf():
        return tree.classification
    return classify(tree.children[data_to_classify[tree.attribute]], data_to_classify)


## score: this method should take as input two lists or Series, one representing the predictions and one representing the true
## values, and return the F1 score. (2 * precision * recall) / (precision + recall) or
##                                   2 * true_positives / (2 * true_positive + false_positive + false_negative)
def score(predicted_results, true_results, positive_class, negative_class):
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    for predicted_result, true_result in zip(predicted_results, true_results):
        if (
                predicted_result == positive_class
                and true_result == positive_class
        ):
            true_positives += 1
        elif (
                predicted_result == positive_class
                and true_result == negative_class
        ):
            false_positives += 1
        elif (
                predicted_result == negative_class
                and true_result == positive_class
        ):
            false_negatives += 1
    return (
            2
            * true_positives
            / (2 * true_positives + false_positives + false_negatives)
    )


## five-fold
def five_fold(data, classifications_column, attr_dict=None):
    ## here is where you'll implement five-fold cross-validation. You should:
    ## 1. split the data into five equal "bins". (If the number is not divisible by 5, that's fine. Some bins can have one extra item.)
    ##  Note that you should not actually copy the data into new structures - just use indices to
    ##   keep track of which data is for training and which is for testing.
    ## 2. You'll do five iterations - for each iteration, 4 of the bins are training, and 1 is the test bin.
    ## In each iteration, create a new classifier and fit it to the training data.
    ##  3. Then test that classifier on the test data and compute F1.
    ## Once you're done, return the five F1 scores.
    data = data.sample(frac=1).reset_index(drop=True)
    f1_scores = []
    k = 5
    fold_size = len(data) // k
    for i in range(k):
        test_start = i * fold_size
        if i == k - 1:
            test_end = len(data)
        else:
            test_end = (i + 1) * fold_size
        test_data = data[test_start:test_end]
        train_data = pd.concat([data[:test_start], data[test_end:]])
        features = train_data.drop(classifications_column, axis=1)
        classifications = train_data[classifications_column]
        node = make_tree(features, classifications, zero_r_val=zero_r(classifications), attr_dict=attr_dict)
        predictions = [classify(node, test_data.iloc[i]) for i in range(len(test_data))]
        f1 = score(predictions, classifications, classifications.unique()[0], classifications.unique()[1])
        f1_scores.append(f1)

    return f1_scores


def main():
    # set up for doctor visits data
    data = pd.read_csv('NPHA-doctor-visits.csv')
    attr_dict = {'Number of Doctors Visited': [1, 2, 3],
                 'Age': [1, 2],
                 'Physical Health': [-1, 1, 2, 3, 4, 5],
                 'Mental Health': [-1, 1, 2, 3, 4, 5],
                 'Dental Health': [-1, 1, 2, 3, 4, 5, 6],
                 'Employment': [-1, 1, 2, 3, 4],
                 'Stress Keeps Patient from Sleeping': [0, 1],
                 'Medication Keeps Patient from Sleeping': [0, 1],
                 'Pain Keeps Patient from Sleeping': [0, 1],
                 'Bathroom Needs Keeps Patient from Sleeping': [0, 1],
                 'Unknown Keeps Patient from Sleeping': [0, 1],
                 'Trouble Sleeping': [-1, 1, 2, 3],
                 'Prescription Sleep Medication': [-1, 1, 2, 3],
                 'Race': [-2, -1, 1, 2, 3, 4, 5],
                 'Gender': [-2, -1, 1, 2]
                 }
    f1_scores = five_fold(data, 'Number of Doctors Visited', attr_dict)
    print(f1_scores)


if __name__ == "__main__":
    main()
